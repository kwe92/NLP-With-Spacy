{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from IPython.core.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAttributes:\n",
    "    def __init__(self,doc):\n",
    "        self.doc = doc\n",
    "        self.token: None\n",
    "        self.lemma: None\n",
    "        self.pos: None\n",
    "        self.synDep: None\n",
    "        self.pos_exp: None\n",
    "        self.synDep_exp: None\n",
    "        self.df: pd.DataFrame\n",
    "        TokenAttributes.__get_attributes(self,doc)\n",
    "    \n",
    "    def __get_attributes(self,doc):\n",
    "        keys = [\"Token\",\"Lemma\",\"Part_of_Speech_Tag\",\"Part_of_Speech_Tag_Explained\", \"Syntactic_Dependency\", \"Syntactic_Dependency_Explained\"]\n",
    "        self.token = [token for token in doc]\n",
    "        self.lemma = [token.lemma_ for token in doc]\n",
    "        self.pos = [token.pos_ for token in doc]\n",
    "        self.synDep = [token.dep_ for token in doc]\n",
    "        self.pos_explain = [spacy.explain(token.pos_) for token in doc]\n",
    "        self.synDep_explain = [spacy.explain(token.dep_) for token in doc]\n",
    "        token_attr = [self.token,self.lemma,self.pos,self.pos_explain,self.synDep,self.synDep_explain]\n",
    "        ta_dict = {k:v for k,v in zip(keys,token_attr)}\n",
    "        self.df = pd.DataFrame(ta_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Pipelines\n",
    "\n",
    "- Statistical Models that enable spaCy to predict linguistic attributes in context\n",
    "    - Part-of-Speech tags\n",
    "    - Syntactic dependencies\n",
    "    - Named entities\n",
    "\n",
    "- Can be trained with more examples to fine-tune predictions\n",
    "\n",
    "- Also contains:\n",
    "    - Binary weights\n",
    "    - Vocabulary\n",
    "    - Meta information\n",
    "    - Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Trained Statistical Pipeline Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- Parsing text into tokens\n",
    "- First step in any NLP application\n",
    "- when you pass text to the statistical model to create a doc object tokenization happens automatically\n",
    "\n",
    "### Text Content\n",
    "\n",
    "- The group of letters that make up the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A statistical model class\n",
    "# A spacy language class\n",
    "# Contains language vocabulary and more!\n",
    "# The predictions are not always right!\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A doc object and the start of a NLP pipeline\n",
    "# instantiate a doc object that will process and tokenize the text\n",
    "# Will also produce predicted lingustic annotations\n",
    "\n",
    "doc = nlp(u\"I am flying to Frisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I am flying to Frisco"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flying"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# doc objects are also itterables and can be sliced like arrays and lists\n",
    "# Each base element of a doc is a token\n",
    "\n",
    "doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'flying', 'to', 'Frisco']\n"
     ]
    }
   ],
   "source": [
    "# Tokens have attributes\n",
    "\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a blank english NLP object\n",
    "# creates a blank statistical model...maybe?\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"I am flying to Frisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'be', 'fly', 'to', 'Frisco']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexical attributes\n",
    "\n",
    "- Lingustic annotations available as token attributes\n",
    "- Check if a token is like a number using the like_num attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\n",
    "        \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "        \"Now less than 4% are.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = doc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "more"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token doc index\n",
    "t1.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boolean check if as token is like a number\n",
    "t1.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 5 60\n",
      "Percentage found: 20 4\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.i, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Part-of-speach Tags\n",
    "- Tells you the part-of-speech of a given word / token\n",
    "- If that token is a noun, verb, adjective etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>PartOfSpeechExplained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>auxiliary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>statistical</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "      <td>particle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>help</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spacy</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>predict</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>context</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>texts</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token PartOfSpeech PartOfSpeechExplained\n",
       "0          This         PRON               pronoun\n",
       "1            is          AUX             auxiliary\n",
       "2             a          DET            determiner\n",
       "3   statistical          ADJ             adjective\n",
       "4         model         NOUN                  noun\n",
       "5            to         PART              particle\n",
       "6          help         VERB                  verb\n",
       "7         spacy         NOUN                  noun\n",
       "8       predict         VERB                  verb\n",
       "9           the          DET            determiner\n",
       "10      context         NOUN                  noun\n",
       "11           of          ADP            adposition\n",
       "12        texts         NOUN                  noun\n",
       "13            !        PUNCT           punctuation"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Trained statistical pipeline that has a .pos_ attribute\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# instantiate a doc object\n",
    "doc = nlp(\"This is a statistical model to help spacy predict the context of texts!\")\n",
    "\n",
    "# list of tokens\n",
    "token = [token for token in doc]\n",
    "\n",
    "# Predicted values for part-of-speech recognition\n",
    "part_of_speech = [token.pos_ for token in doc]\n",
    "\n",
    "token_pos = dict(Token = token, PartOfSpeech = part_of_speech, PartOfSpeechExplained = [spacy.explain(token.pos_) for token in doc])\n",
    "\n",
    "df = pd.DataFrame(data=token_pos)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta = TokenAttributes(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "- The process of reducing word forms to their lemma\n",
    "- lemma\n",
    "    -  The base form of a token\n",
    "- How the token would look in the dictonary\n",
    "- Important task in meaning recognition\n",
    "- Using lemmas can shorten the list of predefined keywords that you need\n",
    "- So that you diont need to include all word forms of a token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammatical Structure of a Sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc objects allow you to access the grammatical strutuce of a sentence\n",
    "\n",
    "doc3 = nlp(u\"This product integrates both libraries for downloading and applying patches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'this'),\n",
       " ('product', 'product'),\n",
       " ('integrates', 'integrate'),\n",
       " ('both', 'both'),\n",
       " ('libraries', 'library'),\n",
       " ('for', 'for'),\n",
       " ('downloading', 'download'),\n",
       " ('and', 'and'),\n",
       " ('applying', 'apply'),\n",
       " ('patches', 'patch')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.text, token.lemma_) for token in doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>integrates</td>\n",
       "      <td>integrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>both</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>libraries</td>\n",
       "      <td>library</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>downloading</td>\n",
       "      <td>download</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>applying</td>\n",
       "      <td>apply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>patches</td>\n",
       "      <td>patch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token      lemma\n",
       "0         This       this\n",
       "1      product    product\n",
       "2   integrates  integrate\n",
       "3         both       both\n",
       "4    libraries    library\n",
       "5          for        for\n",
       "6  downloading   download\n",
       "7          and        and\n",
       "8     applying      apply\n",
       "9      patches      patch"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = [token.text for token in doc3]\n",
    "lemma = [token.lemma_ for token in doc3]\n",
    "\n",
    "pd.DataFrame(dict(token = token, lemma = lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Dependency Label\n",
    " - Syntactic Dependency Parser\n",
    "    -   Describes the syntactic relationship between two words\n",
    " -  Sentence structure formed by directed binary grammatical relations between the tokens\n",
    " - Discover syntactic relations between individual tokens\n",
    " - Connect syntactically related words in a arc / binary tree like structure\n",
    " - Kinda like a binary tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\n",
       "\"source/syntacticDependencyExample1.png\"\n",
       "width=\"900\"\n",
       "height=\"300\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<img src=\n",
    "\"source/syntacticDependencyExample1.png\"\n",
    "width=\"900\"\n",
    "height=\"300\">\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\n",
       "\"source/syntacticDependencyExample2.png\"\n",
       "width=\"1200\"\n",
       "height=\"500\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<img src=\n",
    "\"source/syntacticDependencyExample2.png\"\n",
    "width=\"1200\"\n",
    "height=\"500\">\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I prefer the morning flight through Denver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prefer'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The root node / head / syntactic govener / parent\n",
    "# of the first element\n",
    "\n",
    "doc[0].head.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"Token\",\"Lemma\",\"Part_of_Speech_Tag\",\"Part_of_Speech_Tag_Explained\", \"Syntactic_Dependency\", \"Syntactic_Dependency_Explained\"]\n",
    "\n",
    "token = [token for token in doc] # all tokens of a doc\n",
    "lemma = [token.lemma_ for token in doc] # all lemma's of each token in the doc object\n",
    "pos = [token.pos_ for token in doc] # model predicted part of speech tags for each token in the doc object\n",
    "synDep = [token.dep_ for token in doc] # pipeline predicted syntactic dependency for each token in the doc object\n",
    "pos_explain = [spacy.explain(token.pos_) for token in doc]\n",
    "synDep_explain = [spacy.explain(token.dep_) for token in doc]\n",
    "\n",
    "token_attr = [token,lemma,pos,pos_explain,synDep,synDep_explain]\n",
    "\n",
    "ta_dict = {k:v for k,v in zip(keys,token_attr)}\n",
    "\n",
    "df = pd.DataFrame(ta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Part_of_Speech_Tag</th>\n",
       "      <th>Part_of_Speech_Tag_Explained</th>\n",
       "      <th>Syntactic_Dependency</th>\n",
       "      <th>Syntactic_Dependency_Explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>nominal subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prefer</td>\n",
       "      <td>prefer</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>determiner</td>\n",
       "      <td>det</td>\n",
       "      <td>determiner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morning</td>\n",
       "      <td>morning</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flight</td>\n",
       "      <td>flight</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>dobj</td>\n",
       "      <td>direct object</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>through</td>\n",
       "      <td>through</td>\n",
       "      <td>ADP</td>\n",
       "      <td>adposition</td>\n",
       "      <td>prep</td>\n",
       "      <td>prepositional modifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Denver</td>\n",
       "      <td>Denver</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>pobj</td>\n",
       "      <td>object of preposition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token    Lemma Part_of_Speech_Tag Part_of_Speech_Tag_Explained  \\\n",
       "0        I        I               PRON                      pronoun   \n",
       "1   prefer   prefer               VERB                         verb   \n",
       "2      the      the                DET                   determiner   \n",
       "3  morning  morning               NOUN                         noun   \n",
       "4   flight   flight               NOUN                         noun   \n",
       "5  through  through                ADP                   adposition   \n",
       "6   Denver   Denver              PROPN                  proper noun   \n",
       "\n",
       "  Syntactic_Dependency Syntactic_Dependency_Explained  \n",
       "0                nsubj                nominal subject  \n",
       "1                 ROOT                           root  \n",
       "2                  det                     determiner  \n",
       "3             compound                       compound  \n",
       "4                 dobj                  direct object  \n",
       "5                 prep         prepositional modifier  \n",
       "6                 pobj          object of preposition  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "- A real object that you can refer to by proper name\n",
    "- Like location, organization name, person or other entity\n",
    "- Reveals the place or organization the user is talking about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Apple, U.K., $1 billion)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy.explain Method\n",
    "\n",
    "- Gives you explainations of linguistic attributes\n",
    "- Brief explination on what the label dor tag means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_ent_tags = [\"GPE\",\"NNP\",\"dobj\",\"PROPN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE: Countries, cities, states\n",
      "NNP: noun, proper singular\n",
      "dobj: direct object\n",
      "PROPN: proper noun\n"
     ]
    }
   ],
   "source": [
    "for tag in named_ent_tags:\n",
    "    print(f\"{tag}:\",spacy.explain(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based Matching\n",
    "\n",
    "- Lets you write rules to find words and phrases in text\n",
    "\n",
    "#### Why use Rule-based Matching over RegEx?\n",
    "\n",
    "##### Rule-based Matching allows you to:\n",
    "\n",
    "- Match on doc objects and not just strings\n",
    "\n",
    "- Match on tokens and token attributes\n",
    "\n",
    "- Can search for text and other lexical attributes of a token making RBM more flexible that RegEx\n",
    "\n",
    "- Use a models predictions\n",
    "\n",
    "- Example:\n",
    "    - Find the word duck only if its a noun\n",
    "    - duck(verb) vs duck(noun)\n",
    "\n",
    "\n",
    "##### Match Patterns\n",
    "\n",
    "- A list of dictionaries\n",
    "- One key, value pair dict per token\n",
    "- You can match:\n",
    "\n",
    "    - Exact token text\n",
    "        - [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "        \n",
    "    - Match lexical attributes\n",
    "        - [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "        \n",
    "    - match any token attribute\n",
    "        - [{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher # import the Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.vocab.Vocab at 0x7f8a5be433a0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the matcher object with a sharded vocabulary using nlp.vocab\n",
    "\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process some text / discourse\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Part_of_Speech_Tag</th>\n",
       "      <th>Part_of_Speech_Tag_Explained</th>\n",
       "      <th>Syntactic_Dependency</th>\n",
       "      <th>Syntactic_Dependency_Explained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Upcoming</td>\n",
       "      <td>upcoming</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>adjective</td>\n",
       "      <td>amod</td>\n",
       "      <td>adjectival modifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iPhone</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>proper noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X</td>\n",
       "      <td>x</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>release</td>\n",
       "      <td>release</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>compound</td>\n",
       "      <td>compound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>date</td>\n",
       "      <td>date</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>noun</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>leaked</td>\n",
       "      <td>leak</td>\n",
       "      <td>VERB</td>\n",
       "      <td>verb</td>\n",
       "      <td>acl</td>\n",
       "      <td>clausal modifier of noun (adjectival clause)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token     Lemma Part_of_Speech_Tag Part_of_Speech_Tag_Explained  \\\n",
       "0  Upcoming  upcoming                ADJ                    adjective   \n",
       "1    iPhone    iPhone              PROPN                  proper noun   \n",
       "2         X         x               NOUN                         noun   \n",
       "3   release   release               NOUN                         noun   \n",
       "4      date      date               NOUN                         noun   \n",
       "5    leaked      leak               VERB                         verb   \n",
       "\n",
       "  Syntactic_Dependency                Syntactic_Dependency_Explained  \n",
       "0                 amod                           adjectival modifier  \n",
       "1             compound                                      compound  \n",
       "2             compound                                      compound  \n",
       "3             compound                                      compound  \n",
       "4                 ROOT                                          root  \n",
       "5                  acl  clausal modifier of noun (adjectival clause)  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TokenAttributes(doc).df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Lemmatization - Special Cases\n",
    "\n",
    "-   When there are special cases like nicknames of cities bring passed to the NLP application you can create custom lemma's for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORTH = \"Frisco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMMA = \"San Francisco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I am flying to Frisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I, am, flying, to, Frisco]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'flying', 'to', 'Frisco']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd78fef2128015050713e82ca51c6520b11aee7c9ee8df750520bbbc7384cbaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
